# Snake RL Multi-Agent Evolution System ğŸğŸ§¬âš¡

Un sistema avanzado de entrenamiento de IA que combina el clÃ¡sico juego de la serpiente con algoritmos evolutivos de Ãºltima generaciÃ³n. Entrena hasta 9 agentes simultÃ¡neamente con velocidades de hasta 1200 FPS y un sistema de evoluciÃ³n multi-criterio sofisticado.

## ğŸ¯ CaracterÃ­sticas del Proyecto

### Juego Original
- GrÃ¡ficos coloridos y suaves con pygame
- Sistema de puntuaciÃ³n
- DetecciÃ³n de colisiones
- Controles intuitivos

### Sistema Multi-Agente Evolutivo Avanzado
- **9 Agentes SimultÃ¡neos**: Competencia evolutiva en tiempo real
- **Velocidad Extrema**: Hasta 1200 FPS para entrenamiento ultra-rÃ¡pido
- **EvoluciÃ³n Multi-Criterio**: EvaluaciÃ³n basada en 5 mÃ©tricas diferentes
- **4 Estrategias Evolutivas**: Ã‰lite, crossover, mutaciÃ³n y exploraciÃ³n aleatoria
- **Recompensas Inteligentes**: Sistema que premia acercarse a la comida
- **VisualizaciÃ³n DinÃ¡mica**: Red neuronal del mejor agente en tiempo real
- **Algoritmo REINFORCE**: ImplementaciÃ³n optimizada con PyTorch

## ğŸ“ Estructura del Proyecto

```
rnserpiente/
â”œâ”€â”€ snake_game.py           # Juego original para humanos
â”œâ”€â”€ snake_env.py            # Entorno RL con recompensas inteligentes
â”œâ”€â”€ neural_network.py       # Red neuronal y algoritmo REINFORCE
â”œâ”€â”€ train_agent.py          # Entrenamiento bÃ¡sico individual
â”œâ”€â”€ train_visual.py         # Entrenamiento con visualizaciÃ³n
â”œâ”€â”€ train_multi_visual.py   # Sistema multi-agente evolutivo (PRINCIPAL)
â”œâ”€â”€ test_setup.py           # Script para probar la configuraciÃ³n
â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
â”œâ”€â”€ models/                 # Modelos entrenados guardados
â””â”€â”€ README.md              # Este archivo
```

## ğŸš€ InstalaciÃ³n

1. **Clona o descarga el proyecto**
2. **Instala las dependencias:**

```bash
pip install -r requirements.txt
```

Las dependencias incluyen:
- `pygame`: Para el juego y visualizaciÃ³n
- `torch`: Para la red neuronal
- `numpy`: Para operaciones numÃ©ricas
- `matplotlib`: Para grÃ¡ficos de entrenamiento

## ğŸ® Uso

### 1. Probar la ConfiguraciÃ³n

Antes de entrenar, verifica que todo funcione correctamente:

```bash
python test_setup.py
```

### 2. Jugar el Juego Original (Humano)

```bash
python snake_game.py
```

**Controles:**
- â¬†ï¸â¬‡ï¸â¬…ï¸â¡ï¸ Flechas para mover la serpiente
- **ESPACIO** para reiniciar despuÃ©s de Game Over
- **ESC** para salir

### 3. Entrenar la IA

**Entrenamiento bÃ¡sico (sin visualizaciÃ³n):**
```bash
python train_agent.py --episodes 1000
```

**Entrenamiento con visualizaciÃ³n:**
```bash
python train_agent.py --episodes 1000 --render
```

**Opciones disponibles:**
- `--episodes N`: NÃºmero de episodios de entrenamiento (default: 1000)
- `--test modelo.pth`: Probar un modelo pre-entrenado
- `--test-episodes N`: NÃºmero de episodios para prueba (default: 10)

### 4. Sistema Multi-Agente Evolutivo (RECOMENDADO)

**Entrenamiento con 9 agentes y evoluciÃ³n avanzada:**
```bash
python run_game.py
```
### 5. Probar un Modelo Entrenado

```bash
python train_agent.py --test models/best_snake_model_score_X.pth --test-episodes 5
```

## ğŸ§  CÃ³mo Funciona la IA

### Estado del Juego (Input de la Red Neuronal)
La IA recibe un vector de 14 caracterÃ­sticas:
- **DirecciÃ³n actual** (4 valores one-hot)
- **PosiciÃ³n relativa de la comida** (2 valores normalizados)
- **Peligros en cada direcciÃ³n** (4 valores booleanos)
- **Distancia a las paredes** (4 valores normalizados)

### Red Neuronal
- **Arquitectura**: 14 â†’ 128 â†’ 128 â†’ 128 â†’ 4
- **ActivaciÃ³n**: ReLU + Dropout para regularizaciÃ³n
- **Salida**: Probabilidades de acciÃ³n (Softmax)

### Algoritmo REINFORCE
- **Tipo**: Gradiente de polÃ­tica (Policy Gradient)
- **CaracterÃ­sticas**:
  - ActualizaciÃ³n despuÃ©s de cada episodio completo
  - Retornos descontados con Î³=0.99
  - NormalizaciÃ³n de retornos para estabilidad
  - Gradient clipping para evitar explosiÃ³n de gradientes

### Sistema de Recompensas Inteligente
- **+10**: Por comer comida (objetivo principal)
- **+0.5**: Por acercarse a la comida (guÃ­a direccional)
- **-0.3**: Por alejarse de la comida (desincentivo)
- **-0.1**: Por cada paso (incentiva eficiencia)
- **-10**: Por colisiÃ³n (pared o cuerpo)

## ğŸ§¬ Sistema Evolutivo Multi-Agente

### EvaluaciÃ³n Multi-Criterio (Fitness)
El sistema evalÃºa cada agente usando 5 criterios:
1. **Score Promedio (40%)**: Rendimiento bÃ¡sico
2. **Consistencia (25%)**: Penaliza alta variabilidad
3. **Mejora Progresiva (20%)**: Tendencia ascendente
4. **Eficiencia (10%)**: Reward por step
5. **Supervivencia (5%)**: Episodios sin morir rÃ¡pido

### Estrategias de ReproducciÃ³n
Cada 50 episodios, se crea una nueva generaciÃ³n:
- **Posiciones 1-3**: ğŸ† **Ã‰LITE** - Los 3 mejores se preservan
- **Posiciones 4-6**: ğŸ§¬ **CROSSOVER** - Hijos de cruces entre Ã©lites
- **Posiciones 7-8**: âš¡ **MUTACIÃ“N** - Ã‰lites con mutaciÃ³n fuerte
- **PosiciÃ³n 9**: ğŸ² **EXPLORACIÃ“N** - Agente completamente aleatorio

## ğŸ“Š Monitoreo del Entrenamiento

Durante el entrenamiento verÃ¡s:
- **Score**: Cantidad de comida comida en el episodio
- **Reward**: Recompensa total acumulada
- **Steps**: Pasos dados en el episodio
- **Loss**: PÃ©rdida de la funciÃ³n objetivo
- **Avg Reward (100)**: Promedio mÃ³vil de recompensas

Al finalizar se generan grÃ¡ficos automÃ¡ticamente:
- PuntuaciÃ³n por episodio
- Promedio mÃ³vil de puntuaciÃ³n
- PÃ©rdidas de entrenamiento
- DistribuciÃ³n de puntuaciones

## ğŸ¯ Resultados Esperados

Con el entrenamiento adecuado, la IA deberÃ­a:
- **Episodios 0-200**: Aprender movimientos bÃ¡sicos
- **Episodios 200-500**: Evitar colisiones consistentemente
- **Episodios 500-1000**: Desarrollar estrategias para encontrar comida
- **Episodios 1000+**: Alcanzar puntuaciones de 10+ consistentemente

## ğŸ”§ PersonalizaciÃ³n

### Modificar HiperparÃ¡metros
En `neural_network.py`:
- `learning_rate`: Tasa de aprendizaje (default: 0.001)
- `gamma`: Factor de descuento (default: 0.99)
- `hidden_size`: TamaÃ±o de capas ocultas (default: 128)

### Modificar Recompensas
En `snake_env.py`, mÃ©todo `step()`:
- Cambiar valores de recompensa por comida, colisiÃ³n, etc.

### Modificar Arquitectura de Red
En `neural_network.py`, clase `PolicyNetwork`:
- Agregar/quitar capas
- Cambiar funciones de activaciÃ³n
- Modificar regularizaciÃ³n

## ğŸš¨ SoluciÃ³n de Problemas

**Error de imports:**
- Verifica que todas las dependencias estÃ©n instaladas
- Ejecuta `python test_setup.py` para diagnosticar

**Entrenamiento lento:**
- Entrena sin `--render` para mayor velocidad
- Reduce el nÃºmero de episodios para pruebas rÃ¡pidas

**IA no mejora:**
- Aumenta el nÃºmero de episodios
- Ajusta la tasa de aprendizaje
- Verifica que las recompensas sean apropiadas

## ğŸ“ Conceptos de Aprendizaje por Refuerzo

Este proyecto implementa conceptos clave de RL:
- **PolÃ­tica (Policy)**: La estrategia que sigue el agente
- **Recompensa (Reward)**: SeÃ±al de feedback del entorno
- **Estado (State)**: RepresentaciÃ³n del entorno actual
- **AcciÃ³n (Action)**: DecisiÃ³n tomada por el agente
- **Episodio**: Una partida completa del juego

Â¡Experimenta y diviÃ©rtete aprendiendo sobre IA! ğŸ¤–ğŸ®
